\Appendix
% 付録

\section{使用機材の諸元}
本研究の実験システムを構成するロボットアームおよびカメラの主要な仕様を、表\ref{tab:system_specs}にまとめる。

\begin{table}[htbp]
    \centering
    \caption{実験システムのハードウェア構成}
    \label{tab:system_specs}
    \begin{tabular}{ll}
        \hline
        \textbf{項目} & \textbf{仕様} \\
        \hline
        \multicolumn{2}{l}{\textbf{1. ロボットアーム}} \\
        型番 & RT-CRANE-X7 \\
        自由度 & 7 \\
        作業有効範囲 & 500 mm \\
        可搬重量 & 約 0.5 kg \\
        エンドエフェクタ & 両開きハンド1 \\
        外形寸法 & 130 $\times$ 100 $\times$ 708 mm（固定用金具5mmを含む） \\
        本体重量 & 約 1.5 kg \\
        搭載アクチュエータ & ROBOTIS製 XM540-W270-R, XM430-W350-R \\
        \hline
        \multicolumn{2}{l}{\textbf{2. RGBカメラ}} \\
        型番 & EMEET SmartCam C960 \\
        使用台数 & 2 台 \\
        入力解像度 & 640 $\times$ 480 pixel \\
        フレームレート & 30 fps \\
        画角 (FOV) & 90$^\circ$ \\
        \hline
    \end{tabular}
\end{table}

\subsection{重力補償のキャリブレーションパラメータ}

本研究で導入した重力補償の各関節アクチュエータ（ROBOTIS Dynamixel）におけるトルク電流比および制御パラメータのキャリブレーション結果を参考までに表\ref{tab:gravity_comp_params}に示す。

なお、ベース関節（ID: 2）およびグリッパ部（ID: 8, 9）については、重力補償の適用対象外とした。

\begin{table}[htbp]
    \centering
    \caption{重力補償制御のパラメータ設定}
    \label{tab:gravity_comp_params}
    \begin{tabular}{lc}
        \toprule
        \textbf{対象関節 (ID)} & \textbf{トルク電流換算係数 ($\tau/I$)} \\
        \midrule
        ID: 3 & 0.2267 \\
        ID: 4 & 0.3245 \\
        ID: 5 & 0.3035 \\
        ID: 6 & 0.2545 \\
        ID: 7 & 0.1515 \\
        \bottomrule
    \end{tabular}
\end{table}

\clearpage
\section{キムタオル把持タスクの実験時のロボットの動作遷移}
\setcounter{figure}{0}
\renewcommand{\thefigure}{B.\arabic{figure}}

キムタオル把持タスクにおけるロボットの動作遷移を以下に示す．FigB.1 およびFig B.2 において，同じ記号（\#1～\#8）が付された画像は，側面カメラおよび上面カメラによって同時に撮影されたフレームに対応している．

\begin{figure}[htbp]
  \centering
  % Top row: 4 screenshots
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{tex/Image/kimyoko1.pdf}
    \caption*{\#1 Initial position}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{tex/Image/kimyoko2.pdf}
    \caption*{\#2}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{tex/Image/kimyoko3.pdf}
    \caption*{\#3}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{tex/Image/kimyoko4.pdf}
    \caption*{\#4}
  \end{subfigure}

  \vspace{2mm}

  % Bottom row: 4 screenshots
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{tex/Image/kimyoko5.pdf}
    \caption*{\#5 Grasping}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{tex/Image/kimyoko6.pdf}
    \caption*{\#6 Lifting}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{tex/Image/kimyoko7.pdf}
    \caption*{\#7 Releasing}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{tex/Image/kimyoko8.pdf}
    \caption*{\#8 Task complete}
  \end{subfigure}

  \caption{Sequence of side-view camera images in time order.}
  \label{figB:kimyoko_timeorder}
\end{figure}

\begin{figure}[htbp]
  \centering
  % Top row: 4 top-view screenshots
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{tex/Image/kimue1.pdf}
    \caption*{\#1 Initial position}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{tex/Image/kimue2.pdf}
    \caption*{\#2}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{tex/Image/kimue3.pdf}
    \caption*{\#3}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{tex/Image/kimue4.pdf}
    \caption*{\#4}
  \end{subfigure}

  \vspace{2mm}

  % Bottom row: 4 top-view screenshots
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{tex/Image/kimue5.pdf}
    \caption*{\#5 Grasping}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{tex/Image/kimue6.pdf}
    \caption*{\#6 Lifting}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{tex/Image/kimue7.pdf}
    \caption*{\#7 Releasing}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{tex/Image/kimue8.pdf}
    \caption*{\#8 Task complete}
  \end{subfigure}

  \caption{Sequence of top-view camera images in time order.}
  \label{figB:kimue_timeorder}
\end{figure}



\clearpage
\section{折り紙タスクの実験時のロボットの動作遷移}


\clearpage
\section{触覚を融合したACTモデルの詳細}
特徴抽出と埋め込み
本システムには，RGBカメラ画像、ロボットの関節角度、および指先触覚センサの値が入力される。Transformerは内部計算において、全ての入力データが同一の次元（ベクトル長）を持つことを前提とするため、各モダリティ（情報の種類）に対して以下の前処理を行い、共通の次元 $d_{model}$ を持つ「トークン（特徴ベクトル）」へ変換する。\par

画像情報のエンコード: 複数のカメラから得られたRGB画像は、学習済みの畳み込みニューラルネットワーク（CNN）である ResNet18 等に入力される。CNN は画像からエッジやテクスチャなどの空間的な特徴を抽出する。得られた特徴マップは平坦化（Flatten）された後、線形層（Linear layer）を介して次元 $d_{model}$ へと射影される。\par

状態情報のエンコード: 関節角度や触覚センサの値といった低次元の物理量は、多層パーセプトロン（MLP）等の線形層によって、同様に次元 $d_{model}$ へと射影される。これにより、画像のような高次元データと、センサ値のような低次元データを同一の特徴空間で扱うことが可能となる。\par

位置埋め込み（Positional Embedding）: Transformer 自体はデータの入力順序を認識できない構造であるため、各トークンに対して系列内の位置情報を表すベクトル（位置埋め込み）を加算する。これにより、モデルは「どのデータが何番目の時刻のものか」を区別できるようになる。\par

学習フェーズ
学習時の目的は、与えられた状況（観測）に対してエキスパート（人間）がどのような意図やスタイルで動作したかをモデルに学習させることである。学習時には CVAE エンコーダへ現在の観測と教師データである未来のアクション系列を入力する。\par

[CLS] トークンによる情報の集約: CVAE エンコーダには、入力系列の先頭に [CLS]（Classification Token）と呼ばれる特殊な学習可能トークンを付加する。Transformer の自己注意機構により各トークンは互いの情報を参照し合うが、[CLS] トークンは系列全体の情報を自身に集約するよう学習され、結果として [CLS] トークンは「この観測状況においてどのようなアクションが行われたか」というエピソード全体の特徴（スタイル）を圧縮した表現となる。\par

潜在変数 $z$ の生成: エンコーダの出力から [CLS] トークンに対応するベクトルを抽出し、これを線形層で変換して平均 $\\mu$ と分散 $\\sigma$ を算出する。これらをパラメータとする正規分布から潜在変数 $z$ をサンプリングする。この $z$ は，各状況における動作の「確率的変動要素（例：速度やスタイルの違い）」を表現する。\par

推論フェーズ
推論時および学習時の再構成では、Transformer のエンコーダ・デコーダ構造を用いて、観測データと潜在変数 $z$ から未来のアクションを予測する。\par

観測データの統合（Encoder）: 現在の観測データ（画像・関節・触覚）と CVAE から得られた潜在変数 $z$ を結合し、Transformer エンコーダに入力する。ここで Self-Attention により、例えば「画像内の物体の位置」と「現在の腕の角度」の関係性を紐付けるようなモダリティ間の統合処理が行われる。\par

アクション系列の生成（Decoder）: Transformer デコーダにおけるクエリとして、予測したい未来のステップ数に対応する固定位置埋め込み（Fixed Position Embeddings）を与える。デコーダは Cross-Attention を用いてエンコーダからの情報（Key/Value）を参照しながら各タイムステップのアクション値を算出することで、現在から未来 $k$ ステップ分のアクション系列を一括して出力する。\par

このように、[CLS] トークンを用いた情報の圧縮（CVAE）と Attention 機構によるマルチモーダル情報の統合（Transformer）を組み合わせることで、視覚・固有感覚・触覚を高度に融合したロバストな動作生成が可能となる。