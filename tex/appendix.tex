\Appendix
% 付録

\section*{使用機材の諸元}
本研究の実験システムを構成するロボットアームおよびカメラの主要な仕様を、表\ref{tab:system_specs}にまとめる。

\begin{table}[htbp]
    \centering
    \caption{実験システムのハードウェア構成}
    \label{tab:system_specs}
    \begin{tabular}{ll}
        \hline
        \textbf{項目} & \textbf{仕様} \\
        \hline
        \multicolumn{2}{l}{\textbf{1. ロボットアーム}} \\
        型番 & RT-CRANE-X7 \\
        自由度 & 7 \\
        作業有効範囲 & 500 mm \\
        可搬重量 & 約 0.5 kg \\
        エンドエフェクタ & 両開きハンド1 \\
        外形寸法 & 130 $\times$ 100 $\times$ 708 mm（固定用金具5mmを含む） \\
        本体重量 & 約 1.5 kg \\
        搭載アクチュエータ & ROBOTIS製 XM540-W270-R, XM430-W350-R \\
        \hline
        \multicolumn{2}{l}{\textbf{2. RGBカメラ}} \\
        型番 & EMEET SmartCam C960 \\
        使用台数 & 2 台 \\
        入力解像度 & 640 $\times$ 480 pixel \\
        フレームレート & 30 fps \\
        画角 (FOV) & 90$^\circ$ \\
        \hline
    \end{tabular}
\end{table}

% \subsection*{重力補償のキャリブレーションパラメータ}

% 本研究で導入した重力補償の各関節アクチュエータ（ROBOTIS Dynamixel）におけるトルク電流比および制御パラメータのキャリブレーション結果を参考までに表\ref{tab:gravity_comp_params}に示す。

% なお、ベース関節（ID: 2）およびグリッパ部（ID: 8, 9）については、重力補償の適用対象外とした。

% \begin{table}[htbp]
%     \centering
%     \caption{重力補償制御のパラメータ設定}
%     \label{tab:gravity_comp_params}
%     \begin{tabular}{lc}
%         \toprule
%         \textbf{対象関節 (ID)} & \textbf{トルク電流換算係数 ($\tau/I$)} \\
%         \midrule
%         ID: 3 & 0.2267 \\
%         ID: 4 & 0.3245 \\
%         ID: 5 & 0.3035 \\
%         ID: 6 & 0.2545 \\
%         ID: 7 & 0.1515 \\
%         \bottomrule
%     \end{tabular}
% \end{table}

% 学習の条件は，CPUにAMD Ryzen 9 9950X，GPUにNVIDIA GeForce RTX 5080，メモリ64GBを搭載したPCを使用した．OSはUbuntu 24.04.3 LTS（Kernel 6.14）を用いた．学習のbatch sizeは8，学習ステップ数は20000ステップとした．

% また，センサ値は生データをそのまま使用すると，温度変化によって異なる値を示すため，キャリブレーションを行った．具体的には，各試行の開始時に5秒間静止した状態でセンサ値を取得し，その平均値を基準として各時刻のセンサ値から引き算することで，実験を行う日程や時間帯の変化による温度変化の影響を抑制した．\par

% \clearpage
% \section*{触覚を融合したACTモデルの詳細}
% 特徴抽出と埋め込み
% 本システムには，RGBカメラ画像、ロボットの関節角度、および指先触覚センサの値が入力される。Transformerは内部計算において、全ての入力データが同一の次元（ベクトル長）を持つことを前提とするため、各モダリティ（情報の種類）に対して以下の前処理を行い、共通の次元 $d_{model}$ を持つ「トークン（特徴ベクトル）」へ変換する。\par

% 画像情報のエンコード: 複数のカメラから得られたRGB画像は、学習済みの畳み込みニューラルネットワーク（CNN）である ResNet18 等に入力される。CNN は画像からエッジやテクスチャなどの空間的な特徴を抽出する。得られた特徴マップは平坦化（Flatten）された後、線形層（Linear layer）を介して次元 $d_{model}$ へと射影される。\par

% 状態情報のエンコード: 関節角度や触覚センサの値といった低次元の物理量は、多層パーセプトロン（MLP）等の線形層によって、同様に次元 $d_{model}$ へと射影される。これにより、画像のような高次元データと、センサ値のような低次元データを同一の特徴空間で扱うことが可能となる。\par

% 位置埋め込み（Positional Embedding）: Transformer 自体はデータの入力順序を認識できない構造であるため、各トークンに対して系列内の位置情報を表すベクトル（位置埋め込み）を加算する。これにより、モデルは「どのデータが何番目の時刻のものか」を区別できるようになる。\par

% 学習フェーズ
% 学習時の目的は、与えられた状況（観測）に対してエキスパート（人間）がどのような意図やスタイルで動作したかをモデルに学習させることである。学習時には CVAE エンコーダへ現在の観測と教師データである未来のアクション系列を入力する。\par

% [CLS] トークンによる情報の集約: CVAE エンコーダには、入力系列の先頭に [CLS]（Classification Token）と呼ばれる特殊な学習可能トークンを付加する。Transformer の自己注意機構により各トークンは互いの情報を参照し合うが、[CLS] トークンは系列全体の情報を自身に集約するよう学習され、結果として [CLS] トークンは「この観測状況においてどのようなアクションが行われたか」というエピソード全体の特徴（スタイル）を圧縮した表現となる。\par

% 潜在変数 $z$ の生成: エンコーダの出力から [CLS] トークンに対応するベクトルを抽出し、これを線形層で変換して平均 $\\mu$ と分散 $\\sigma$ を算出する。これらをパラメータとする正規分布から潜在変数 $z$ をサンプリングする。この $z$ は，各状況における動作の「確率的変動要素（例：速度やスタイルの違い）」を表現する。\par

% 推論フェーズ
% 推論時および学習時の再構成では、Transformer のエンコーダ・デコーダ構造を用いて、観測データと潜在変数 $z$ から未来のアクションを予測する。\par

% 観測データの統合（Encoder）: 現在の観測データ（画像・関節・触覚）と CVAE から得られた潜在変数 $z$ を結合し、Transformer エンコーダに入力する。ここで Self-Attention により、例えば「画像内の物体の位置」と「現在の腕の角度」の関係性を紐付けるようなモダリティ間の統合処理が行われる。\par

% アクション系列の生成（Decoder）: Transformer デコーダにおけるクエリとして、予測したい未来のステップ数に対応する固定位置埋め込み（Fixed Position Embeddings）を与える。デコーダは Cross-Attention を用いてエンコーダからの情報（Key/Value）を参照しながら各タイムステップのアクション値を算出することで、現在から未来 $k$ ステップ分のアクション系列を一括して出力する。\par

% このように、[CLS] トークンを用いた情報の圧縮（CVAE）と Attention 機構によるマルチモーダル情報の統合（Transformer）を組み合わせることで、視覚・固有感覚・触覚を高度に融合したロバストな動作生成が可能となる。